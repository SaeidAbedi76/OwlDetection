{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128,"status":"ok","timestamp":1710979468357,"user":{"displayName":"Saeid Abedi","userId":"05966781143170182634"},"user_tz":240},"id":"vU_ZUewDUdW7","outputId":"045df1a5-3e03-4bce-d86a-ac629f7be981"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/MyDrive/OwlDetection/'\n","/content/drive/MyDrive/OwlDetection\n"]}],"source":["cd drive/MyDrive/OwlDetection/"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":269,"status":"ok","timestamp":1710980111446,"user":{"displayName":"Saeid Abedi","userId":"05966781143170182634"},"user_tz":240},"id":"8bQTNGJOU_Uc"},"outputs":[],"source":["!git config --global user.email \"sabedi4@uwo.ca\"\n","!git config --global user.name \"saeidabedi76\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPbYDJXobEJt","outputId":"591c426a-c292-40ae-d83f-af3ee512bf54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 923 images belonging to 6 classes.\n","Found 30 images belonging to 6 classes.\n","Found 30 images belonging to 6 classes.\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","\n","# Set some parameters\n","img_height, img_width = 224, 224\n","batch_size = 32\n","\n","# Set up the training, validation, and test directories\n","train_dir = '/content/drive/MyDrive/OwlDetection/train'\n","validation_dir = '/content/drive/MyDrive/OwlDetection/valid'\n","test_dir = '/content/drive/MyDrive/OwlDetection/test'\n","\n","# Set up data augmentation configuration for training\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","# Note that validation and test data should not be augmented!\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Set up the data generators\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical'  # 'categorical' because we have more than 2 classes\n",")\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    validation_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","\n","test_generator = test_datagen.flow_from_directory(\n","    test_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    shuffle=False  # Do not shuffle test data\n",")\n","\n","# Helper function to plot images\n","def plot_images(images_arr, titles):\n","    fig, axes = plt.subplots(1, len(images_arr), figsize=(20,20))\n","    axes = axes.flatten()\n","    for img, ax, title in zip(images_arr, axes, titles):\n","        ax.imshow(img)\n","        ax.axis('off')\n","        ax.set_title(title)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Displaying images from each generator\n","# Extract and plot a batch from each\n","train_images, _ = next(train_generator)\n","validation_images, _ = next(validation_generator)\n","test_images, _ = next(test_generator)\n","\n","# We take the first three images of the batch\n","plot_images(train_images[:3], ['Train1', 'Train2', 'Train3'])\n","plot_images(validation_images[:3], ['Validation1', 'Validation2', 'Validation3'])\n","plot_images(test_images[:3], ['Test1', 'Test2', 'Test3'])\n"]},{"cell_type":"markdown","metadata":{},"source":["CLIP Zero-shot as the standard of the other model's accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","import torch\n","import os\n","\n","# Load CLIP model and processor from Hugging Face Transformers\n","model_id = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_id)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","\n","# Directory containing your dataset\n","dataset_path = train_dir  # Update this path if needed\n","\n","# Class labels\n","class_folders = ['BARN OWL', 'GREAT GRAY OWL', 'LONG-EARED OWL', 'ORIENTAL BAY OWL', 'SNOWY OWL', 'STRIPED OWL']\n","\n","# Initialize counters\n","correct_predictions = 0\n","total_images = 0\n","\n","# Loop through each class folder\n","for class_name in class_folders:\n","    folder_path = os.path.join(dataset_path, class_name)\n","\n","    # Check if the folder exists\n","    if not os.path.isdir(folder_path):\n","        print(f\"Folder {folder_path} does not exist.\")\n","        continue\n","\n","    for image_name in os.listdir(folder_path):\n","        # Prepare the image and text inputs\n","        image_path = os.path.join(folder_path, image_name)\n","        try:\n","            image = Image.open(image_path).convert(\"RGB\")\n","        except (IOError, FileNotFoundError):\n","            print(f\"Error opening image {image_path}. Skipping.\")\n","            continue\n","\n","        inputs = processor(text=class_folders, images=image, return_tensors=\"pt\", padding=True)\n","\n","        # Get model outputs\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        # Process outputs (calculate probabilities and find the best match)\n","        logits_per_image = outputs.logits_per_image\n","        probs = logits_per_image.softmax(dim=1)\n","        top_result = probs.argmax()\n","\n","        # Check prediction\n","        predicted_class = class_folders[top_result]\n","        if predicted_class == class_name:\n","            correct_predictions += 1\n","        total_images += 1\n","\n","        print(f\"Image: {image_name}, Actual class: {class_name}, Predicted: {predicted_class}\")\n","\n","# Calculate accuracy\n","accuracy = correct_predictions / total_images if total_images > 0 else 0\n","print(f\"Overall accuracy: {accuracy * 100:.2f}%\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNJm0zBgY6kpTzfCEkX8XiC","mount_file_id":"1wpgPNDYfraScwGIRL9kgBRdj1p18w7Ud","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
